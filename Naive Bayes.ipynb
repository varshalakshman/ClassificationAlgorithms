{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Enter the name of the data file:project3_dataset1.txt\n",
      "Correct data file found and imported\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enter the data file name, open it and assign it to the variable data_file\n",
    "print (\"Naive Bayes\")\n",
    "while True:\n",
    "    try:\n",
    "        filename = input(\"Enter the name of the data file:\")       \n",
    "        \n",
    "    # In case of a error the except block will be executed\n",
    "    except:\n",
    "        print(\"Error: Could not find file or read data\")\n",
    "        continue\n",
    "        \n",
    "    # This block will be executed after correct data file name is entered\n",
    "    else:\n",
    "        print(\"Correct data file found and imported\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as file:\n",
    "        matrix = [line.rstrip().split('\\t') for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array(matrix)\n",
    "output_class = matrix[:,-1].reshape((matrix.shape[0],1)).astype(int)\n",
    "matrix = matrix[:,0:-1]\n",
    "#print(matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check if attribute is String or not\n",
    "def is_string(s):\n",
    "    try:\n",
    "        complex(s) # for int, long, float and complex\n",
    "    except ValueError:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_indices = []\n",
    "i=0\n",
    "while i < len(matrix[0]):\n",
    "    if is_string(matrix[0][i]):\n",
    "        s_indices.append(i)\n",
    "    i+=1       \n",
    "for i in s_indices:\n",
    "    unique_strings = np.unique(matrix[:,i])\n",
    "    replacement_vals = list(range(len(unique_strings)))\n",
    "    dictionary = dict(zip(unique_strings, replacement_vals))\n",
    "    j=0\n",
    "    while j < len(matrix[:,i]):\n",
    "        matrix[j][i] = dictionary.get(matrix[j][i])\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending the output class at the last \n",
    "matrix = matrix.astype(float)\n",
    "matrix = np.append(matrix,output_class,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probability_zero(matrix):\n",
    "    column = list(matrix[:,-1]).count(0)\n",
    "    return float(column)/len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probability_one(matrix):\n",
    "    column = list(matrix[:,-1]).count(1)\n",
    "    return float(column)/len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of zeros\n",
    "def meanzero(train_float):\n",
    "        mean_zero=[]\n",
    "        for row in train_float:\n",
    "               if row[-1] == 0:\n",
    "                    mean_zero.append(row[0:-1]) \n",
    "                \n",
    "        return np.mean(mean_zero,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of ones\n",
    "def meanone(train_float):\n",
    "        mean_one=[]\n",
    "        for row in train_float:\n",
    "               if row[-1] == 1:\n",
    "                    mean_one.append(row[0:-1]) \n",
    "                \n",
    "        return np.mean(mean_one,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviation of zeros\n",
    "def stdzero(train_float):\n",
    "        std_zero = []\n",
    "        for row in train_float:\n",
    "               if row[-1] == 0:\n",
    "                      std_zero.append(row[0:-1]) \n",
    "        return np.std(std_zero,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = np.array_split(matrix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviation of ones\n",
    "def stdone(train_float):\n",
    "    return np.std([row[0:-1] for row in train_float if row[-1] == 1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  calculate_categorical(class_zero,class_one,test,train):\n",
    "    string_prob_zero = np.empty(test.shape[0])\n",
    "    string_prob_one = np.empty(test.shape[0])\n",
    "    if len(s_indices) == 0:\n",
    "        string_test = 0\n",
    "    elif len(s_indices) != 0:\n",
    "        string_prior_prob_one = {}\n",
    "        string_prior_prob_zero = {}    \n",
    "        \n",
    "        for j in s_indices:\n",
    "            string_prior_prob_one[j] = {}\n",
    "            string_prior_prob_zero[j] = {}\n",
    "            for k in np.unique(train[:,j]): \n",
    "                prior_one = float(list(class_one[:,j]).count(k))/list(class_one[:,-1].astype(int)).count(1) \n",
    "                prior_zero = float(list(class_zero[:,j]).count(k))/list(class_zero[:,-1].astype(int)).count(0)\n",
    "                string_prior_prob_zero[j][k] = prior_zero\n",
    "                string_prior_prob_one[j][k] = prior_one\n",
    "       \n",
    "    string_prob_one.fill(1.0)\n",
    "    string_prob_zero.fill(1.0)\n",
    " \n",
    "    if len(s_indices) != 0:\n",
    "        t = 0\n",
    "        while t < len(test):\n",
    "            for i in s_indices:\n",
    "                string_prob_one[t] *= string_prior_prob_one[i][test[t][i]]\n",
    "                string_prob_zero[t] *= string_prior_prob_zero[i][test[t][i]] \n",
    "                t+=1\n",
    "    return string_prob_one,string_prob_zero             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(test, train):\n",
    "    test_class = []\n",
    "    \n",
    "    class_zero = []\n",
    "    class_one = []\n",
    "    for j in range(len(train)):\n",
    "        if int(train[j,-1]) == 0:\n",
    "            class_zero.append(train[j,:])\n",
    "        elif int(train[j,-1]) == 1:\n",
    "            class_one.append(train[j,:])\n",
    "\n",
    "    class_zero = np.asarray(class_zero)\n",
    "    class_one = np.asarray(class_one)\n",
    "    \n",
    "    #Categorical Data\n",
    "    string_prob_one,string_prob_zero = calculate_categorical(class_zero,class_one,test,train)\n",
    "\n",
    "    #Calculating prior probabilities of zeros and ones\n",
    "    prior_probablity_zero = prior_probability_zero(train) \n",
    "    prior_probablity_one = prior_probability_one(train) \n",
    "\n",
    "    #Excluding categorical\n",
    "    final_itemset =[]\n",
    "    for i in range(len(train[0])):\n",
    "         if i not in s_indices:\n",
    "                final_itemset.append(train[:,i])\n",
    "    train_float = np.transpose(np.asarray(final_itemset))  \n",
    "    \n",
    "    mean_zero = meanzero(train_float)  \n",
    "    mean_one = meanone(train_float) \n",
    "\n",
    "    std_zero = stdzero(train_float)      \n",
    "    std_one =  stdone(train_float)      \n",
    "\n",
    "    testing_float =[]\n",
    "    for i in range(len(test[0])):\n",
    "         if i not in s_indices:\n",
    "                testing_float.append(test[:,i])\n",
    "    test_float = np.transpose(np.asarray(testing_float))  \n",
    "\n",
    "    #Actual probability of zero and one\n",
    "    prob_zero = prior_probablity_zero * np.multiply(np.prod(np.exp(-1 * np.multiply(test_float[:,0:-1] - mean_zero,test_float[:,0:-1] - mean_zero) / (2 * np.multiply(std_zero,std_zero)))/(math.sqrt(math.pi*2)*std_zero), axis = 1), string_prob_zero)\n",
    "    prob_one = prior_probablity_one * np.multiply(np.prod(np.exp(-1 * np.multiply(test_float[:,0:-1] - mean_one,test_float[:,0:-1] - mean_one) / (2 * np.multiply(std_one,std_one)))/(math.sqrt(math.pi*2)*std_one), axis = 1), string_prob_one)\n",
    "\n",
    "    #Decide which test data point belongs to which class\n",
    "    test_class = []\n",
    "    for i in range(len(test)):\n",
    "                if prob_one[i] > prob_zero[i]:\n",
    "                       test_class.append(1)\n",
    "                elif prob_one[i] < prob_zero[i]:\n",
    "                       test_class.append(0)\n",
    "    return test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the performance parameters and Getting metrics - TruePositive, FalsePositive, TrueNegative, FalseNegative for calculating accuracy/precision/recall/f-measure\n",
    "def performance_metrics(actual, predicted):\n",
    "    tp = tn = fp = fn = 0\n",
    "    for i in range(len(actual)):\n",
    "\n",
    "        if actual[i] == 1 and predicted[i] == 1:\n",
    "            tp += 1\n",
    "        if actual[i] == 1 and predicted[i] == 0:\n",
    "            fn += 1\n",
    "        if actual[i] == 0 and predicted[i] == 1:\n",
    "            fp += 1\n",
    "        if actual[i] == 0 and predicted[i] == 0:\n",
    "            tn += 1\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    F1 = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    if tp + fp != 0:\n",
    "        precision = tp / float(tp + fp)\n",
    "    if tp + fn != 0:\n",
    "        recall = tp / float(fn + tp)\n",
    "    if tp + tn + fp + fn != 0:\n",
    "        accuracy = (tp + tn) / float(tp + tn + fp + fn)\n",
    "    if precision + recall != 0:\n",
    "        F1 = (2 * precision * recall) / float(precision + recall)\n",
    "\n",
    "    precision = precision * 100\n",
    "    recall = recall * 100\n",
    "    accuracy = accuracy * 100\n",
    "    F1 = F1 * 100\n",
    "    print(\"----------\")\n",
    "    print(accuracy)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(F1)\n",
    "    print(\"---------\")\n",
    "    return {'precision': precision, 'recall': recall, 'accuracy': accuracy, 'F1': F1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "94.73684210526315\n",
      "91.66666666666666\n",
      "95.65217391304348\n",
      "93.61702127659574\n",
      "---------\n",
      "----------\n",
      "92.98245614035088\n",
      "88.23529411764706\n",
      "88.23529411764706\n",
      "88.23529411764706\n",
      "---------\n",
      "----------\n",
      "96.49122807017544\n",
      "87.5\n",
      "100.0\n",
      "93.33333333333333\n",
      "---------\n",
      "----------\n",
      "91.22807017543859\n",
      "90.0\n",
      "85.71428571428571\n",
      "87.80487804878048\n",
      "---------\n",
      "----------\n",
      "89.47368421052632\n",
      "88.88888888888889\n",
      "80.0\n",
      "84.21052631578948\n",
      "---------\n",
      "----------\n",
      "96.49122807017544\n",
      "100.0\n",
      "92.5925925925926\n",
      "96.15384615384615\n",
      "---------\n",
      "----------\n",
      "96.49122807017544\n",
      "95.45454545454545\n",
      "95.45454545454545\n",
      "95.45454545454545\n",
      "---------\n",
      "----------\n",
      "96.49122807017544\n",
      "93.33333333333333\n",
      "93.33333333333333\n",
      "93.33333333333333\n",
      "---------\n",
      "----------\n",
      "91.22807017543859\n",
      "96.42857142857143\n",
      "87.09677419354838\n",
      "91.52542372881356\n",
      "---------\n",
      "----------\n",
      "89.28571428571429\n",
      "86.36363636363636\n",
      "86.36363636363636\n",
      "86.36363636363636\n",
      "---------\n",
      "Final Accuracy: 93.490%\n",
      "Final Precision: 91.787%\n",
      "Final Recall: 90.444%\n",
      "Final F1: 91.003%\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(len(cross_validation)):\n",
    "    test_set = cross_validation[i]\n",
    "    train_set=[]\n",
    "    for l,x in enumerate(cross_validation):\n",
    "         if l != i:\n",
    "            train_set.append(x)\n",
    "    train_set = np.vstack(train_set)\n",
    "    test_set = np.asarray(test_set)\n",
    "    train_set = np.asarray(train_set)\n",
    "    class_list = calculate_probability(test_set, train_set)\n",
    "    \n",
    "    actual_class = test_set[:,-1]\n",
    "    predicted_class = np.asarray(class_list)\n",
    "    accuracy = performance_metrics(actual_class, predicted_class)\n",
    "    scores.append(accuracy)\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "accuracy = 0\n",
    "F1 = 0\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    precision += scores[i]['precision']\n",
    "    recall += scores[i]['recall']\n",
    "    accuracy += scores[i]['accuracy']\n",
    "    F1 += scores[i]['F1']\n",
    "\n",
    "print('Final Accuracy: %.3f%%' % (accuracy/float(len(scores))))\n",
    "print('Final Precision: %.3f%%' % (precision/float(len(scores))))\n",
    "print('Final Recall: %.3f%%' % (recall/float(len(scores))))\n",
    "print('Final F1: %.3f%%' % (F1/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
